{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae203d90bcceab73",
   "metadata": {},
   "source": [
    "### 1. Asymmetric Linear Quantization from scratch: FP32 -> Lower bits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee716b613197240",
   "metadata": {},
   "source": [
    "In this notebook we will go through simple linear quantization methodology. Our goal is to compress ML model from __32-bit floating point__ precision to a __lower one__ preserving the most of the model acuracy.\n",
    "\n",
    "In general quantization approach helps to reduce excessive use of memory to store, read and apply ML models.\n",
    "\n",
    "__Linear Quantization__ suggests that we can store quantized model weights along with small amount of parameters that helps to restore quantized weights as much as possible into the original form with higher bit precision via set of linear operations. The simplest way to do that is to store some scaling coefficients and track zero point between higher and lower precision weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af38a8633778b7",
   "metadata": {},
   "source": [
    "In __Asymmetric Linear quantization__ we track quantized weights __scaling coefficients__ and __zero point__ location for a quantized range of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a03df72a3e9317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from watermark import watermark\n",
    "\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6383de0-241a-42e7-b9d6-d774fce85eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2025-04-05T01:05:03.163055+03:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.12.9\n",
      "IPython version      : 9.0.2\n",
      "\n",
      "Compiler    : GCC 11.2.0\n",
      "OS          : Linux\n",
      "Release     : 5.15.0-136-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 12\n",
      "Architecture: 64bit\n",
      "\n",
      "numpy     : 2.2.4\n",
      "sklearn   : 1.6.1\n",
      "matplotlib: 3.10.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(watermark())\n",
    "print(watermark(packages=\"numpy,sklearn,matplotlib\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05e0ec0-a735-479c-9ede-e945d79adb7c",
   "metadata": {},
   "source": [
    "In this example we will use wine dataset and neural network as kinds of wine classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3223a46-7557-453f-9d9e-3f700f27f16f",
   "metadata": {},
   "source": [
    "#### Preparing example dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e6e6ae-2916-44a7-ba7e-1c5a4a1ebc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "test_size = 0.2\n",
    "\n",
    "np.random.seed(seed)\n",
    "data = datasets.load_wine()\n",
    "\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "y_train_enc = enc.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_enc = enc.transform(y_test.reshape(-1, 1))\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438ff183-2a76-4a7a-aa2f-928d9404a1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142, 13) (142, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_sc.shape, y_train_enc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a80928-402f-4aa5-aa12-b21fd81dd623",
   "metadata": {},
   "source": [
    "#### Preparing neural network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ea71b5-ba7a-4549-b51e-e87bc5c65b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet:\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size: int, \n",
    "        hidden_size: int, \n",
    "        output_size: int\n",
    "    ):\n",
    "        # input_size, number of features in our dataset\n",
    "        # hidden_size, hidden feature vector\n",
    "        # output_size, the number of classes in our task\n",
    "\n",
    "        # First layer with bias\n",
    "        self.W1 = np.random.randn(input_size, hidden_size).astype(np.float32)\n",
    "        self.b1 = np.random.randn(1, hidden_size).astype(np.float32)\n",
    "        \n",
    "        # Second layer with bias\n",
    "        self.W2 = np.random.randn(hidden_size, output_size).astype(np.float32)\n",
    "        self.b2 = np.random.randn(1, output_size).astype(np.float32)\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        # Apply weights: z1 = xW1 + b1\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        # Apply ReLU activation function\n",
    "        self.a1 = np.maximum(0, self.z1)\n",
    "        # Apply weights: z2 = xW2 + b2\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "\n",
    "        # Aply softmax activation\n",
    "        exp_z2 = np.exp(self.z2)\n",
    "        self.a2 = exp_z2 / np.sum(exp_z2, axis=1, keepdims=True)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X: np.ndarray, y: np.ndarray, learning_rate: float) -> None:\n",
    "\n",
    "        m = X.shape[0]\n",
    "\n",
    "        dz2 = self.a2 - y\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        \n",
    "        dz1 = da1 * (self.z1 > 0)  # Gradient for ReLU\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Update weights with fixed learning_rate\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42fe35f3-49dc-46ce-9b39-2f0b93bb36f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model parameters\n",
    "n_features = X.shape[1]\n",
    "input_size = n_features\n",
    "hidden_size = 100\n",
    "output_size = len(np.unique(y))\n",
    "\n",
    "# Set model training configuration\n",
    "epochs = 200\n",
    "batch_size = 10\n",
    "lr = 0.1\n",
    "eps = 1e-8\n",
    "show_epochs = 10\n",
    "train_size = len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790cc63b-c879-4f51-a6b2-f7a9834b4ab5",
   "metadata": {},
   "source": [
    "Lets train our simple classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d77fd2a9-bff4-49a9-94bb-07bdd88776be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Train loss: 47.7868022582481\n",
      "Epoch: 10. Train loss: 0.0031307487465227204\n",
      "Epoch: 20. Train loss: 0.0017780614852699924\n",
      "Epoch: 30. Train loss: 0.001298985522747322\n",
      "Epoch: 40. Train loss: 0.0010395265328199085\n",
      "Epoch: 50. Train loss: 0.0008721869063203863\n",
      "Epoch: 60. Train loss: 0.0007540463131598365\n",
      "Epoch: 70. Train loss: 0.0006655551491054781\n",
      "Epoch: 80. Train loss: 0.0005966229607126343\n",
      "Epoch: 90. Train loss: 0.0005411552106892298\n",
      "Epoch: 100. Train loss: 0.0004954622940959396\n",
      "Epoch: 110. Train loss: 0.0004571125890286121\n",
      "Epoch: 120. Train loss: 0.00042442829149903983\n",
      "Epoch: 130. Train loss: 0.00039621811734276543\n",
      "Epoch: 140. Train loss: 0.00037160832295757436\n",
      "Epoch: 150. Train loss: 0.00034993862444279976\n",
      "Epoch: 160. Train loss: 0.00033070528697895854\n",
      "Epoch: 170. Train loss: 0.0003135132574756775\n",
      "Epoch: 180. Train loss: 0.0002980508060238217\n",
      "Epoch: 190. Train loss: 0.000284067189271836\n"
     ]
    }
   ],
   "source": [
    "model = NNet(input_size, hidden_size, output_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  loss_accum = 0\n",
    "  for i in range(0, train_size, batch_size):\n",
    "    X_batch = X_train_sc[i: i + batch_size, :]\n",
    "    y_batch = np.array(y_train_enc[i: i + batch_size, :].todense())\n",
    "\n",
    "    y_pred = model.forward(X_batch)\n",
    "    ce_loss = -np.mean( np.sum(np.multiply(y_batch, np.log(y_pred + eps)), axis=1) )\n",
    "    model.backward(X_batch, y_batch, lr)\n",
    "    loss_accum += ce_loss\n",
    "  if epoch % show_epochs == 0:\n",
    "    print(f\"Epoch: {epoch}. Train loss: {loss_accum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d157a62-9759-4591-95e7-34b969a2c4ef",
   "metadata": {},
   "source": [
    "Evaluate model on a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c8afbe5-7781-4b07-8c7a-7db1dc294dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       1.00      1.00      1.00        14\n",
      "     class_1       1.00      1.00      1.00        14\n",
      "     class_2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = np.argmax(model.forward(X_test_sc), axis=1)\n",
    "print(classification_report(y_test, y_pred_test, target_names=data.target_names))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef0d4b0-14a6-4cec-b3d9-9549ce3871bc",
   "metadata": {},
   "source": [
    "Everybody wishes to get such metrics on a real data 😄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e08a88b-f2f6-45b2-befd-fe9c687f1493",
   "metadata": {},
   "source": [
    "Now when we have the prepared model we can apply quantization. But before doing that lets measure current model storage assuming that we have 32-bit floating point weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c55c1ba-bcbc-4a0b-9a32-c543005f818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_storage(model_obj: object, num_bits: int) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the total storage size of a model in bytes.\n",
    "    NB!!! Without storage need for scaling coefficients and zero point\n",
    "\n",
    "    Parameters:\n",
    "        model (TwoLayerNet): The neural network model with weights and biases.\n",
    "        num_bits (int): The bit precision (e.g., 32 for FP32, 8 for INT8).\n",
    "\n",
    "    Returns:\n",
    "        int: Total storage size in bytes.\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "\n",
    "    # Count parameters for each layer\n",
    "    total_params += np.prod(model_obj.W1.shape) + np.prod(model_obj.b1.shape)\n",
    "    total_params += np.prod(model_obj.W2.shape) + np.prod(model_obj.b2.shape)\n",
    "\n",
    "    # Calculate storage in bytes\n",
    "    total_bits = total_params * num_bits\n",
    "    total_bytes = total_bits // 8\n",
    "\n",
    "    return total_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "023d15be-a58c-405d-a0de-f9cc357b36c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model size is 6.65KB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial model size is {calculate_model_storage(model, 32) / 1024:.2f}KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16b23060-1087-4238-abf5-9c02aac3aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will store initial results for further comparison\n",
    "RESULTS = {\n",
    "    \"fp32\": [1.00, 6.65, 0] # Macro F1, Model size in KB, MSE between initial weights and dequantized ones\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f614ad-45e9-4f68-b282-4d07aa19407b",
   "metadata": {},
   "source": [
    "### Apply linear quantization\n",
    "\n",
    "\n",
    "We will stick to the following procedure to pick the best model from the quantized candidates:\n",
    "\n",
    "1. Apply initial cross-layer equalization, CLE\n",
    "2. Apply linear quantization\n",
    "3. Calculate MSE between initial FP32 weights and dequantized weights calculated from lower precision\n",
    "4. Evaluate model storage and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56ba29ed-babf-4d3e-825e-bfd78a22e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_layer_equalization(W1: np.ndarray, W2: np.ndarray, b1: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Normalize the weights across two consecutive layers to balance their scales.\n",
    "    Adjusts W1, W2, and b1 in place to equalize the activation ranges.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_per_neuron = np.max(np.abs(W1), axis=0)  # Max value per neuron in W1\n",
    "    scaling_factors = max_per_neuron / np.max(max_per_neuron)  # Normalize scaling\n",
    "\n",
    "    # Adjust W1 and b1\n",
    "    W1 /= scaling_factors\n",
    "    b1 /= scaling_factors\n",
    "\n",
    "    # Compensate in W2\n",
    "    W2 *= scaling_factors.reshape(-1, 1)\n",
    "\n",
    "    return W1, W2, b1\n",
    "     \n",
    "\n",
    "def quantize_tensor(tensor: np.ndarray, num_bits: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Apply assymetric linear quantization operation\n",
    "    \"\"\"\n",
    "\n",
    "    qmin = -(2 ** (num_bits - 1))     # Minimum value in quantized range\n",
    "    qmax = (2 ** (num_bits - 1)) - 1. # Maximum value in quantized range\n",
    "\n",
    "    min_val = np.min(tensor)          # Minimum real value of a tensor\n",
    "    max_val = np.max(tensor)          # Maximum real value of a tensor\n",
    "\n",
    "    scale = (max_val - min_val) / (qmax - qmin) # Scaling factor\n",
    "    zero_point = qmin - min_val / scale         # Zero point location for a quantized range\n",
    "\n",
    "    # These matrices can be stored along with scaling factors and zero point for dequantization\n",
    "    quantized = np.round(zero_point + tensor / scale).clip(qmin, qmax).astype(int)\n",
    "    dequantized = scale * (quantized - zero_point)\n",
    "\n",
    "    return quantized, dequantized, scale, zero_point\n",
    "\n",
    "\n",
    "def get_quant_mse(x1: np.ndarray, x2: np.ndarray, precision: int = 4) -> float:\n",
    "  return np.round(np.sum( (x1 - x2)**2), precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a54d104-da46-4e3c-8752-8449b9035f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_preicions_in_bits = (8, 6, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46b2e67d-4079-436e-91dc-8e81e97011d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Qunatization with Bits: 8\n",
      "#Original FP32 W1:\n",
      "[[ 0.7547377  -0.31663933  0.31212547 ...  0.15707463 -1.2657021\n",
      "  -1.021201  ]\n",
      " [-3.2211206  -0.63667977 -0.47048968 ...  0.46516895  0.58665264\n",
      "  -3.0792668 ]\n",
      " [ 0.8575016   0.7706897   1.961209   ...  0.61525416  3.1439714\n",
      "   1.7663364 ]\n",
      " ...\n",
      " [ 3.1434956   1.5586464  -0.27126428 ... -3.799705    2.5548832\n",
      "   0.82688206]\n",
      " [ 2.1054003  -3.799705    3.7997048  ...  1.5569623   0.27901947\n",
      "   3.6949296 ]\n",
      " [-0.33069095 -0.5592149  -0.60601294 ... -0.7499743  -0.2171366\n",
      "   0.553019  ]]\n",
      "#Quantized W1 with 8 bits:\n",
      "[[  25  -11   10 ...    5  -43  -35]\n",
      " [-109  -22  -16 ...   15   19 -104]\n",
      " [  28   25   65 ...   20  105   59]\n",
      " ...\n",
      " [ 105   52  -10 ... -128   85   27]\n",
      " [  70 -128  127 ...   52    9  123]\n",
      " [ -12  -19  -21 ...  -26   -8   18]]\n",
      "#Dequantized W1:\n",
      "[[ 0.7599413  -0.31291669  0.31291714 ...  0.16390909 -1.26656823\n",
      "  -1.02815535]\n",
      " [-3.23347455 -0.64073441 -0.46192474 ...  0.4619252   0.58113164\n",
      "  -3.0844665 ]\n",
      " [ 0.84934614  0.7599413   1.95200574 ...  0.61093325  3.14407017\n",
      "   1.77319607]\n",
      " ...\n",
      " [ 3.14407017  1.5645848  -0.28311508 ... -3.79970516  2.54803796\n",
      "   0.81954453]\n",
      " [ 2.10101379 -3.79970516  3.79970561 ...  1.5645848   0.28311553\n",
      "   3.68049917]\n",
      " [-0.3427183  -0.55132957 -0.6109328  ... -0.75994085 -0.22351185\n",
      "   0.55133003]]\n",
      "Quantization MSE: 0.1081\n",
      "INT8 Model Storage: 1.66 KB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       1.00      1.00      1.00        14\n",
      "     class_1       1.00      1.00      1.00        14\n",
      "     class_2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n",
      "### Qunatization with Bits: 6\n",
      "#Original FP32 W1:\n",
      "[[ 0.7547377  -0.31663933  0.31212547 ...  0.15707463 -1.2657021\n",
      "  -1.021201  ]\n",
      " [-3.2211206  -0.63667977 -0.47048968 ...  0.46516895  0.58665264\n",
      "  -3.0792668 ]\n",
      " [ 0.8575016   0.7706897   1.961209   ...  0.61525416  3.1439714\n",
      "   1.7663364 ]\n",
      " ...\n",
      " [ 3.1434956   1.5586464  -0.27126428 ... -3.799705    2.5548832\n",
      "   0.82688206]\n",
      " [ 2.1054003  -3.799705    3.7997048  ...  1.5569623   0.27901947\n",
      "   3.6949296 ]\n",
      " [-0.33069095 -0.5592149  -0.60601294 ... -0.7499743  -0.2171366\n",
      "   0.553019  ]]\n",
      "#Quantized W1 with 6 bits:\n",
      "[[  6  -3   2 ...   1 -11  -9]\n",
      " [-27  -6  -4 ...   3   4 -26]\n",
      " [  7   6  16 ...   5  26  14]\n",
      " ...\n",
      " [ 26  12  -3 ... -32  21   6]\n",
      " [ 17 -32  31 ...  12   2  30]\n",
      " [ -3  -5  -6 ...  -7  -2   4]]\n",
      "#Dequantized W1:\n",
      "[[ 0.78406616 -0.30156391  0.30156391 ...  0.18093834 -1.26656841\n",
      "  -1.02531729]\n",
      " [-3.19657742 -0.6634406  -0.42218947 ...  0.42218947  0.54281503\n",
      "  -3.07595186]\n",
      " [ 0.90469172  0.78406616  1.99032179 ...  0.6634406   3.19657742\n",
      "   1.74907066]\n",
      " ...\n",
      " [ 3.19657742  1.50781954 -0.30156391 ... -3.79970523  2.5934496\n",
      "   0.78406616]\n",
      " [ 2.11094735 -3.79970523  3.79970523 ...  1.50781954  0.30156391\n",
      "   3.67907967]\n",
      " [-0.30156391 -0.54281503 -0.6634406  ... -0.78406616 -0.18093834\n",
      "   0.54281503]]\n",
      "Quantization MSE: 1.7699\n",
      "INT6 Model Storage: 1.25 KB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       1.00      1.00      1.00        14\n",
      "     class_1       1.00      1.00      1.00        14\n",
      "     class_2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n",
      "### Qunatization with Bits: 4\n",
      "#Original FP32 W1:\n",
      "[[ 0.7547377  -0.31663933  0.31212547 ...  0.15707463 -1.2657021\n",
      "  -1.021201  ]\n",
      " [-3.2211206  -0.63667977 -0.47048968 ...  0.46516895  0.58665264\n",
      "  -3.0792668 ]\n",
      " [ 0.8575016   0.7706897   1.961209   ...  0.61525416  3.1439714\n",
      "   1.7663364 ]\n",
      " ...\n",
      " [ 3.1434956   1.5586464  -0.27126428 ... -3.799705    2.5548832\n",
      "   0.82688206]\n",
      " [ 2.1054003  -3.799705    3.7997048  ...  1.5569623   0.27901947\n",
      "   3.6949296 ]\n",
      " [-0.33069095 -0.5592149  -0.60601294 ... -0.7499743  -0.2171366\n",
      "   0.553019  ]]\n",
      "#Quantized W1 with 4 bits:\n",
      "[[ 1 -1  0 ...  0 -3 -3]\n",
      " [-7 -2 -1 ...  0  1 -7]\n",
      " [ 1  1  3 ...  1  6  3]\n",
      " ...\n",
      " [ 6  3 -1 ... -8  5  1]\n",
      " [ 4 -8  7 ...  3  0  7]\n",
      " [-1 -2 -2 ... -2 -1  1]]\n",
      "#Dequantized W1:\n",
      "[[ 0.75994107 -0.25331369  0.25331369 ...  0.25331369 -1.26656845\n",
      "  -1.26656845]\n",
      " [-3.29307798 -0.75994107 -0.25331369 ...  0.25331369  0.75994107\n",
      "  -3.29307798]\n",
      " [ 0.75994107  0.75994107  1.77319583 ...  0.75994107  3.29307798\n",
      "   1.77319583]\n",
      " ...\n",
      " [ 3.29307798  1.77319583 -0.25331369 ... -3.79970536  2.78645059\n",
      "   0.75994107]\n",
      " [ 2.27982321 -3.79970536  3.79970536 ...  1.77319583  0.25331369\n",
      "   3.79970536]\n",
      " [-0.25331369 -0.75994107 -0.75994107 ... -0.75994107 -0.25331369\n",
      "   0.75994107]]\n",
      "Quantization MSE: 31.1959\n",
      "INT4 Model Storage: 0.83 KB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       1.00      1.00      1.00        14\n",
      "     class_1       1.00      1.00      1.00        14\n",
      "     class_2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n",
      "### Qunatization with Bits: 2\n",
      "#Original FP32 W1:\n",
      "[[ 0.7547377  -0.31663933  0.31212547 ...  0.15707463 -1.2657021\n",
      "  -1.021201  ]\n",
      " [-3.2211206  -0.63667977 -0.47048968 ...  0.46516895  0.58665264\n",
      "  -3.0792668 ]\n",
      " [ 0.8575016   0.7706897   1.961209   ...  0.61525416  3.1439714\n",
      "   1.7663364 ]\n",
      " ...\n",
      " [ 3.1434956   1.5586464  -0.27126428 ... -3.799705    2.5548832\n",
      "   0.82688206]\n",
      " [ 2.1054003  -3.799705    3.7997048  ...  1.5569623   0.27901947\n",
      "   3.6949296 ]\n",
      " [-0.33069095 -0.5592149  -0.60601294 ... -0.7499743  -0.2171366\n",
      "   0.553019  ]]\n",
      "#Quantized W1 with 2 bits:\n",
      "[[ 0 -1  0 ...  0 -1 -1]\n",
      " [-2 -1 -1 ...  0  0 -2]\n",
      " [ 0  0  0 ...  0  1  0]\n",
      " ...\n",
      " [ 1  0 -1 ... -2  1  0]\n",
      " [ 0 -2  1 ...  0  0  1]\n",
      " [-1 -1 -1 ... -1 -1  0]]\n",
      "#Dequantized W1:\n",
      "[[ 1.26656842 -1.26656842  1.26656842 ...  1.26656842 -1.26656842\n",
      "  -1.26656842]\n",
      " [-3.79970527 -1.26656842 -1.26656842 ...  1.26656842  1.26656842\n",
      "  -3.79970527]\n",
      " [ 1.26656842  1.26656842  1.26656842 ...  1.26656842  3.79970527\n",
      "   1.26656842]\n",
      " ...\n",
      " [ 3.79970527  1.26656842 -1.26656842 ... -3.79970527  3.79970527\n",
      "   1.26656842]\n",
      " [ 1.26656842 -3.79970527  3.79970527 ...  1.26656842  1.26656842\n",
      "   3.79970527]\n",
      " [-1.26656842 -1.26656842 -1.26656842 ... -1.26656842 -1.26656842\n",
      "   1.26656842]]\n",
      "Quantization MSE: 786.4405\n",
      "INT2 Model Storage: 0.42 KB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.82      1.00      0.90        14\n",
      "     class_1       1.00      0.79      0.88        14\n",
      "     class_2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.92        36\n",
      "   macro avg       0.94      0.93      0.93        36\n",
      "weighted avg       0.93      0.92      0.92        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for bits in weights_preicions_in_bits:\n",
    "\n",
    "  print(f\"### Qunatization with Bits: {bits}\")\n",
    "\n",
    "  model_quantized = copy.deepcopy(model)\n",
    "\n",
    "  # Step 1: Apply CLE\n",
    "  W1, W2, b1 = cross_layer_equalization(model_quantized.W1, model_quantized.W2, model_quantized.b1)\n",
    "  b2 = model_quantized.b2\n",
    "\n",
    "  # Step 2: Quantize weights and biases of the model\n",
    "  q_W1, dq_W1, scale_W1, zp_W1 = quantize_tensor(W1, bits)\n",
    "  q_b1, dq_b1, scale_b1, zp_b1 = quantize_tensor(b1, bits)\n",
    "  q_W2, dq_W2, scale_W2, zp_W2 = quantize_tensor(W2, bits)\n",
    "  q_b2, dq_b2, scale_b2, zp_b2 = quantize_tensor(b2, bits)\n",
    "  print(f\"#Original FP32 W1:\\n{W1}\\n#Quantized W1 with {bits} bits:\\n{q_W1}\\n#Dequantized W1:\\n{dq_W1}\")\n",
    "  quant_mse = np.round(get_quant_mse(dq_W1, W1) + get_quant_mse(dq_W2, W2) + get_quant_mse(dq_b2, b2) + get_quant_mse(dq_b1, b1), 4)\n",
    "  print(f\"Quantization MSE: {quant_mse}\")\n",
    "\n",
    "  # Step 3: Assign new weights to the model\n",
    "  model_quantized.W1 = dq_W1\n",
    "  model_quantized.b1 = dq_b1\n",
    "  model_quantized.W2 = dq_W2\n",
    "  model_quantized.b2 = dq_b2\n",
    "\n",
    "  storage_kb = calculate_model_storage(model, num_bits=bits) / 1024\n",
    "  print(f\"INT{bits} Model Storage: {storage_kb :.2f} KB\")\n",
    "  y_pred_test_quant = np.argmax(model_quantized.forward(X_test_sc), axis=1)\n",
    "  report = classification_report(y_test, y_pred_test_quant, target_names=data.target_names)\n",
    "  report_dict = classification_report(y_test, y_pred_test_quant, target_names=data.target_names, output_dict=True)\n",
    "  print(report)\n",
    "\n",
    "  RESULTS.update(\n",
    "      {\n",
    "          f\"Q_{bits}bit\": [\n",
    "              float(report_dict[\"macro avg\"][\"f1-score\"]),\n",
    "              float(storage_kb),\n",
    "              float(quant_mse),\n",
    "          ]\n",
    "      }\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a06cf5b-fa3b-438b-ba5b-0f68604d7df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fp32': [1.0, 6.65, 0],\n",
       " 'Q_8bit': [1.0, 1.6630859375, 0.1081],\n",
       " 'Q_6bit': [1.0, 1.2470703125, 1.7699],\n",
       " 'Q_4bit': [1.0, 0.8310546875, 31.1959],\n",
       " 'Q_2bit': [0.927741935483871, 0.4150390625, 786.4405]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8027c4a4-e8da-4045-91e4-8faa5d0eeef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(RESULTS, index=[\"macro_avg_f1\", \"Kb\", \"MSE\"]).reset_index()\n",
    "df_results = df_results.rename(columns={\"index\": \"Weights\"})\n",
    "df_results = df_results.T.reset_index()\n",
    "df_results  = pd.DataFrame(df_results.values[1:], columns=df_results.iloc[0])\n",
    "\n",
    "# Simple efficiency metric\n",
    "# It shows how much model accuracy nominated in `macro_avg_f1` we store per one Kb of the model storage\n",
    "df_results[\"macro_avg_f1_per_Kb\"] = df_results[\"macro_avg_f1\"] / df_results[\"Kb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ae665d0-570b-456c-86ce-d28142cdd1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weights</th>\n",
       "      <th>macro_avg_f1</th>\n",
       "      <th>Kb</th>\n",
       "      <th>MSE</th>\n",
       "      <th>macro_avg_f1_per_Kb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fp32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q_8bit</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.663086</td>\n",
       "      <td>0.1081</td>\n",
       "      <td>0.601292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q_6bit</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.24707</td>\n",
       "      <td>1.7699</td>\n",
       "      <td>0.801879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q_4bit</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.831055</td>\n",
       "      <td>31.1959</td>\n",
       "      <td>1.20329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q_2bit</td>\n",
       "      <td>0.927742</td>\n",
       "      <td>0.415039</td>\n",
       "      <td>786.4405</td>\n",
       "      <td>2.235312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0 Weights macro_avg_f1        Kb       MSE macro_avg_f1_per_Kb\n",
       "0    fp32          1.0      6.65       0.0            0.150376\n",
       "1  Q_8bit          1.0  1.663086    0.1081            0.601292\n",
       "2  Q_6bit          1.0   1.24707    1.7699            0.801879\n",
       "3  Q_4bit          1.0  0.831055   31.1959             1.20329\n",
       "4  Q_2bit     0.927742  0.415039  786.4405            2.235312"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c0785-7ac2-473a-b7fd-f075767fa5cd",
   "metadata": {},
   "source": [
    "On top of that, we should keep the following linear coefficients for later weights dequantization during inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46f1602d-7e71-4499-806e-aa9c727db060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scale_W1': np.float32(2.5331368),\n",
       " 'zp_W1': np.float32(-0.5),\n",
       " 'scale_b1': np.float32(3.3597019),\n",
       " 'zp_b1': np.float32(-0.7340083),\n",
       " 'scale_W2': np.float32(1.3195125),\n",
       " 'zp_W2': np.float32(-0.4996891),\n",
       " 'scale_b2': np.float32(0.067290716),\n",
       " 'zp_b2': np.float32(-8.078146)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_coefficients = {\n",
    "    \"scale_W1\": scale_W1,\n",
    "    \"zp_W1\": zp_W1,\n",
    "    \"scale_b1\": scale_b1,\n",
    "    \"zp_b1\": zp_b1,\n",
    "    \"scale_W2\": scale_W2,\n",
    "    \"zp_W2\": zp_W2,\n",
    "    \"scale_b2\": scale_b2,\n",
    "    \"zp_b2\": zp_b2\n",
    "}\n",
    "linear_coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149eaa5-4f9d-4fcb-ae77-8ee17a0c2fc6",
   "metadata": {},
   "source": [
    "Choosing the best model depends on the specific circumstances. For example, if we want to balance model accuracy described via `macro_avg_f1` and model storage efficiency, we can pick `Q_4bit` solution. If we need to use as less memory as possible and sacrifice some accuracy for that, `Q_2bit` is our choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca9cedd-d022-46bf-b858-a2f4417c48fa",
   "metadata": {},
   "source": [
    "That's it! We created a wine classifier with Numpy from scratch and applied asymmetric linear quantization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbbd740-ecdb-435a-8375-36c017ee4c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
